{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import duckdb as db\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_referralfile(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Expects csv file type with full file path, returns a spark data frame\"\"\"\n",
    "    \n",
    "    df = pl.read_csv(file=file_path)\n",
    "\n",
    "    df_renamed = df.rename(\n",
    "        {\n",
    "            'Referring':'Referring Provider'\n",
    "            , 'Referring_duplicated_0': 'Referring Provider NPI'\n",
    "            , 'Referral': 'Referral Date'\n",
    "            , 'pat': 'pat Status'\n",
    "            , 'Referred to': 'Referred to Specialist'\n",
    "            , 'Specialist': 'Specialist NPI'\n",
    "            , 'Visit': 'Visit Status'\n",
    "            , 'Health': 'Health Plan'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Drop first row\n",
    "    df_drop_rows = df_renamed[1:, :]\n",
    "\n",
    "    # Create new column 'Update_DT' where value is seperated DT in the next row \n",
    "    df_fill = df_drop_rows.with_columns(Update_DT = df_drop_rows['Last Update'].shift(-1))\n",
    "\n",
    "    # Add column for file_name\n",
    "    df_name = df_fill.with_columns(file_source = pl.lit(file_path))\n",
    "\n",
    "    # Create list from str\n",
    "    df_list = df_name.with_columns(pl.col('Diagnosis').str.split(','))\n",
    "\n",
    "    # Create struct from list\n",
    "    df_strct = df_list.with_columns(pl.col('Diagnosis').arr.to_struct())\n",
    "\n",
    "    df_expl = df_strct.unnest('Diagnosis').rename({'field_0': 'Diagnosis'})\n",
    "\n",
    "    # drop all null values now that we've cleaned the data\n",
    "    df_clean = df_expl.filter(~pl.all(pl.col('Center').is_null()))\n",
    "\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(spark_df: pl.DataFrame, file_path: str):\n",
    "    \"\"\"writes a spark dataframe to parquet file path\"\"\"\n",
    "\n",
    "    spark_df.write_parquet(file=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_write_file(inc_file_path: str, out_file_path):\n",
    "    \"\"\"wrapper function for cleaning and writing referral files to parquet\"\"\"\n",
    "\n",
    "    clean = clean_referralfile(inc_file_path)\n",
    "    write_parquet(clean, out_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file_names(in_path: str = 'data/raw_referrals/'):\n",
    "    \"\"\"cleans the file names within 'data/raw_referrals/ dir\"\"\"\n",
    "    \n",
    "    for file_name in os.listdir(in_path):\n",
    "        f_name = file_name.upper().replace(' - ', '-').replace('TEXAS', 'TX')\n",
    "        os.rename(in_path+file_name, in_path+f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_clean_write_files(in_path: str = 'data/raw_referrals/', out_path: str = 'data/clean_referrals/'):\n",
    "    \"\"\"iteratively cleans file names then writes to clean_referrals dir as parquet\"\"\"\n",
    "\n",
    "    for file_name in os.listdir(in_path):\n",
    "        f_name = file_name[:-4]\n",
    "        clean_write_file(in_path+file_name, out_path+f_name+'.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_write_file('data/raw_referrals/FL referrals 3.1-3.29.csv', 'data/clean_referrals/fl_2023_03_01-2023_03_29.parquet')\n",
    "clean_write_file('data/raw_referrals/TX Referrals 3.1-3.29.csv', 'data/clean_referrals/tx_2023_03_01-2023_03_29.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Data to DuckDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = db.connect(\"data/referral.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
