{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import duckdb as db\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Process\n",
    "## Transforming Data\n",
    "- Processess responsible for reading and cleaning data\n",
    "    - Expects referral Excel files to turn into Apache Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_referralfile(file_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Expects excel file type with full file path, returns a spark data frame\"\"\"\n",
    "    \n",
    "    df = pl.read_excel(file=file_path, xlsx2csv_options={\"dateformat\": \"%Y-%m-%d\"})\n",
    "    df_renamed = df.rename(\n",
    "        {\n",
    "            'Referring':'Referring Provider'\n",
    "            , 'Referring_duplicated_0': 'Referring Provider NPI'\n",
    "            , 'Referral': 'Referral Date'\n",
    "            , 'pat': 'pat Status'\n",
    "            , 'Referred to': 'Referred to Specialist'\n",
    "            , 'Specialist': 'Specialist NPI'\n",
    "            , 'Visit': 'Visit Status'\n",
    "            , 'Health': 'Health Plan'\n",
    "        }\n",
    "    )\n",
    "    # Drop first row\n",
    "    df_drop_rows = df_renamed[1:, :]\n",
    "    # Create new column 'Update_DT' where value is seperated DT in the next row \n",
    "    df_fill = df_drop_rows.with_columns(Update_DT = df_drop_rows['Last Update'].shift(-1))\n",
    "    # Add column for file_name\n",
    "    df_name = df_fill.with_columns(file_source = pl.lit(file_path))\n",
    "    # Create struct from list from str\n",
    "    df_list = df_name.with_columns(pl.col('Diagnosis').str.split(',').arr.to_struct())\n",
    "    df_expl = df_list.unnest('Diagnosis').rename({'field_0': 'Diagnosis'})\n",
    "    # upper case and strip whitespace from text\n",
    "    df_clean_text = df_expl.with_columns(\n",
    "        (pl.col('Last Name').str.to_uppercase().str.strip())\n",
    "        , (pl.col('First Name').str.to_uppercase().str.strip())\n",
    "        , (pl.col('Last Update').str.to_uppercase().str.split(','))\n",
    "        , (pl.col('Health Plan').str.to_uppercase().str.strip())\n",
    "    )\n",
    "    df_user = df_clean_text.with_columns((pl.col('Last Update').arr.last().str.strip()).alias(\"User_Fname\"), (pl.col('Last Update').arr.first().str.strip()).alias(\"User_LName\"))\n",
    "    df_recomb_user = df_user.with_columns(pl.col('Last Update').arr.join(', '))\n",
    "    # drop all null values now that we've cleaned the data\n",
    "    df_clean = df_recomb_user.filter(~pl.all(pl.col('Center').is_null()))\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(spark_df: pl.DataFrame, file_path: str):\n",
    "    \"\"\"writes a spark dataframe to parquet file path\"\"\"\n",
    "\n",
    "    spark_df.write_parquet(file=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_write_file(inc_file_path: str, out_file_path):\n",
    "    \"\"\"wrapper function for cleaning and writing referral files to parquet\"\"\"\n",
    "\n",
    "    clean = clean_referralfile(inc_file_path)\n",
    "    write_parquet(clean, out_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file_names(in_path: str):\n",
    "    \"\"\"cleans the file names within 'data/raw_referrals/ dir\"\"\"\n",
    "    \n",
    "    for file_name in os.listdir(in_path):\n",
    "        f_ext = file_name[-5:]\n",
    "        f_name = file_name[:-5]\n",
    "        f_name = f_name.upper().replace(' - ', '-').replace('TEXAS', 'TX').replace('.', '_')\n",
    "        os.rename(in_path+file_name, in_path+f_name+f_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_clean_write_files(in_path: str = 'data/raw_referrals/excels/', out_path: str = 'data/clean_referrals/'):\n",
    "    \"\"\"iteratively cleans file names then writes to clean_referrals dir as parquet\"\"\"\n",
    "\n",
    "    clean_file_names(in_path)\n",
    "\n",
    "    for file_name in os.listdir(in_path):\n",
    "        f_name = file_name[:-5]\n",
    "        clean_write_file(in_path+file_name, out_path+f_name+'.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_clean_write_files('../data/raw_referrals/excels/', '../data/clean_referrals/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation -> Reporting Data\n",
    "## Creating Database & Tables\n",
    "- Processes responsible for creating or replacing database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_commit(query: str):\n",
    "    db.connect('../data/referral.db')\n",
    "    db.sql(query=query)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_team():\n",
    "    with open('queries/create_tables/user_list.sql', 'r') as query:\n",
    "        query_commit(query=query.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coc():\n",
    "    with open('queries/create_tables/coc.sql', 'r') as query:\n",
    "        query_commit(query=query.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hcpcs():\n",
    "    with open('queries/create_tables/hcpcs.sql', 'r') as query:\n",
    "        query_commit(query=query.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_icd10():\n",
    "    with open('queries/create_tables/icd10cm.sql', 'r') as query:\n",
    "        query_commit(query=query.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_referrals():\n",
    "    with open('queries/create_tables/referrals.sql', 'r') as query:\n",
    "        query_commit(query=query.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_referrals():\n",
    "    with open('queries/bi_referrals.sql', 'r') as query:\n",
    "        query_commit(query=query.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_export():\n",
    "    today = str(datetime.date.today())\n",
    "    to_str = f\"COPY (SELECT * FROM bi_referrals) TO '../data/report_data/{today}.parquet' (FORMAT 'parquet')\"\n",
    "    query_commit(query=to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tables():\n",
    "    create_team()\n",
    "    create_hcpcs()\n",
    "    create_icd10()\n",
    "    create_icd10()\n",
    "    create_referrals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_export():\n",
    "    transform_referrals()\n",
    "    create_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_data():\n",
    "    initialize_tables()\n",
    "    transform_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0739d05adc443d864c1fd60b01b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d500f67d5cb496d9665dd4decae24b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='100%'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_report_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ab04dd0eab1ca903bd011a9e0b61e0588f996ed7771f48df6be986b19ab4641"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
